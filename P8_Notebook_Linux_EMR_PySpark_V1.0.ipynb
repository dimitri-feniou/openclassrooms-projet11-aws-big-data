{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441159cc",
   "metadata": {},
   "source": [
    "# D√©ployez un mod√®le dans le cloud\n",
    "\n",
    "\n",
    "# Sommaire :\n",
    "\n",
    "**1. Pr√©ambule**<br />\n",
    "&emsp;1.1 Probl√©matique<br />\n",
    "&emsp;1.2 Objectifs dans ce projet<br />\n",
    "&emsp;1.3 D√©roulement des √©tapes du projet<br />\n",
    "**2. Choix techniques g√©n√©raux retenus**<br />\n",
    "&emsp;2.1 Calcul distribu√©<br />\n",
    "&emsp;2.2 Transfert Learning<br />\n",
    "**3. D√©ploiement de la solution en local**<br />\n",
    "&emsp;3.1 Environnement de travail<br />\n",
    "&emsp;3.2 Installation de Spark<br />\n",
    "&emsp;3.3 Installation des packages<br />\n",
    "&emsp;3.4 Import des librairies<br />\n",
    "&emsp;3.5 D√©finition des PATH pour charger les images et enregistrer les r√©sultats<br />\n",
    "&emsp;3.6 Cr√©ation de la SparkSession<br />\n",
    "&emsp;3.7 Traitement des donn√©es<br />\n",
    "&emsp;&emsp;3.7.1 Chargement des donn√©es<br />\n",
    "&emsp;&emsp;3.7.2 Pr√©paration du mod√®le<br />\n",
    "&emsp;&emsp;3.7.3 D√©finition du processus de chargement des images et application <br />\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;de leur featurisation √† travers l'utilisation de pandas UDF<br />\n",
    "&emsp;&emsp;3.7.4 Ex√©cution des actions d'extractions de features<br />\n",
    "&emsp;3.8 Chargement des donn√©es enregistr√©es et validation du r√©sultat<br />\n",
    "**4. D√©ploiement de la solution sur le cloud**<br />\n",
    "&emsp;4.1 Choix du prestataire cloud : AWS<br />\n",
    "&emsp;4.2 Choix de la solution technique : EMR<br />\n",
    "&emsp;4.3 Choix de la solution de stockage des donn√©es : Amazon S3<br />\n",
    "&emsp;4.4 Configuration de l'environnement de travail<br />\n",
    "&emsp;4.5 Upload de nos donn√©es sur S3<br />\n",
    "&emsp;4.6 Configuration du serveur EMR<br />\n",
    "&emsp;&emsp;4.6.1 √âtape 1 : Logiciels et √©tapes<br />\n",
    "&emsp;&emsp;&emsp;4.6.1.1 Configuration des logiciels<br />\n",
    "&emsp;&emsp;&emsp;4.6.1.2 Modifier les param√®tres du logiciel<br />\n",
    "&emsp;&emsp;4.6.2 √âtape 2 : Mat√©riel<br />\n",
    "&emsp;&emsp;4.6.3 √âtape 3 : Param√®tres de cluster g√©n√©raux<br />\n",
    "&emsp;&emsp;&emsp;4.6.3.1 Options g√©n√©rales<br />\n",
    "&emsp;&emsp;&emsp;4.6.3.2 Actions d'amor√ßage<br />\n",
    "&emsp;&emsp;4.6.4 √âtape 4 : S√©curit√©<br />\n",
    "&emsp;&emsp;&emsp;4.6.4.1 Options de s√©curit√©<br />\n",
    "&emsp;4.7 Instanciation du serveur<br />\n",
    "&emsp;4.8 Cr√©ation du tunnel SSH √† l'instance EC2 (Ma√Ætre)<br />\n",
    "&emsp;&emsp;4.8.1 Cr√©ation des autorisations sur les connexions entrantes<br />\n",
    "&emsp;&emsp;4.8.2 Cr√©ation du tunnel ssh vers le Driver<br />\n",
    "&emsp;&emsp;4.8.3 Configuration de FoxyProxy<br />\n",
    "&emsp;&emsp;4.8.4 Acc√®s aux applications du serveur EMR via le tunnel ssh<br />\n",
    "&emsp;4.9 Connexion au notebook JupyterHub<br />\n",
    "&emsp;4.10 Ex√©cution du code<br />\n",
    "&emsp;&emsp;4.10.1 D√©marrage de la session Spark<br />\n",
    "&emsp;&emsp;4.10.2 Installation des packages<br />\n",
    "&emsp;&emsp;4.10.3 Import des librairies<br />\n",
    "&emsp;&emsp;4.10.4 D√©finition des PATH pour charger les images et enregistrer les r√©sultats<br />\n",
    "&emsp;&emsp;4.10.5 Traitement des donn√©es<br />\n",
    "&emsp;&emsp;&emsp;4.10.5.1 Chargement des donn√©es<br />\n",
    "&emsp;&emsp;&emsp;4.10.5.2 Pr√©paration du mod√®le<br />\n",
    "&emsp;&emsp;&emsp;4.10.5.3 D√©finition du processus de chargement des images<br />\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;et application de leur featurisation √† travers l'utilisation de pandas UDF<br />\n",
    "&emsp;&emsp;&emsp;4.10.5.4 Ex√©cutions des actions d'extractions de features<br />\n",
    "&emsp;&emsp;4.10.6 Chargement des donn√©es enregistr√©es et validation du r√©sultat<br />\n",
    "&emsp;4.11 Suivi de l'avancement des t√¢ches avec le Serveur d'Historique Spark<br />\n",
    "&emsp;4.12 R√©siliation de l'instance EMR<br />\n",
    "&emsp;4.13 Cloner le serveur EMR (si besoin)<br />\n",
    "&emsp;4.14 Arborescence du serveur S3 √† la fin du projet<br />\n",
    "**5. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cee08",
   "metadata": {},
   "source": [
    "# 1. Pr√©ambule\n",
    "\n",
    "## 1.1 Probl√©matique\n",
    "\n",
    "La tr√®s jeune start-up de l'AgriTech, nomm√©e \"**Fruits**!\", <br />\n",
    "cherche √† proposer des solutions innovantes pour la r√©colte des fruits.\n",
    "\n",
    "La volont√© de l‚Äôentreprise est de pr√©server la biodiversit√© des fruits <br />\n",
    "en permettant des traitements sp√©cifiques pour chaque esp√®ce de fruits <br />\n",
    "en d√©veloppant des robots cueilleurs intelligents.\n",
    "\n",
    "La start-up souhaite dans un premier temps se faire conna√Ætre en mettant <br />\n",
    "√† disposition du grand public une application mobile qui permettrait aux <br />\n",
    "utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n",
    "\n",
    "Pour la start-up, cette application permettrait de sensibiliser le grand public <br /> \n",
    "√† la biodiversit√© des fruits et de mettre en place une premi√®re version du moteur <br />\n",
    "de classification des images de fruits.\n",
    "\n",
    "De plus, le d√©veloppement de l‚Äôapplication mobile permettra de construire <br />\n",
    "une premi√®re version de l'architecture **Big Data** n√©cessaire.\n",
    "\n",
    "## 1.2 Objectifs dans ce projet\n",
    "\n",
    "1. D√©velopper une premi√®re cha√Æne de traitement des donn√©es qui <br />\n",
    "   comprendra le **preprocessing** et une √©tape de **r√©duction de dimension**.\n",
    "2. Tenir compte du fait que <u>le volume de donn√©es va augmenter <br />\n",
    "   tr√®s rapidement</u> apr√®s la livraison de ce projet, ce qui implique de:\n",
    " - D√©ployer le traitement des donn√©es dans un environnement **Big Data**\n",
    " - D√©velopper les scripts en **pyspark** pour effectuer du **calcul distribu√©**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95e6ce",
   "metadata": {},
   "source": [
    "## 1.3 D√©roulement des √©tapes du projet\n",
    "\n",
    "Le projet va √™tre r√©alis√© en 2 temps, dans deux environnements diff√©rents. <br />\n",
    "Nous allons dans un premier temps d√©velopper et ex√©cuter notre code en local, <br />\n",
    "en travaillant sur un nombre limit√© d'images √† traiter.\n",
    "\n",
    "Une fois les choix techniques valid√©s, nous d√©ploierons notre solution <br />\n",
    "dans un environnement Big Data en mode distribu√©.\n",
    "\n",
    "<u>Pour cette raison, ce projet sera divis√© en 3 parties</u>:\n",
    "1. Liste des choix techniques g√©n√©raux retenus\n",
    "2. D√©ploiement de la solution en local\n",
    "3. D√©ploiement de la solution dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34029",
   "metadata": {},
   "source": [
    "# 2. Choix techniques g√©n√©raux retenus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32baf092",
   "metadata": {},
   "source": [
    "## 2.1 Calcul distribu√©\n",
    "\n",
    "L‚Äô√©nonc√© du projet nous impose de d√©velopper des scripts en **pyspark** <br />\n",
    "afin de <u>prendre en compte l‚Äôaugmentation tr√®s rapide du volume <br />\n",
    "de donn√© apr√®s la livraison du projet</u>.\n",
    "\n",
    "Pour comprendre rapidement et simplement ce qu‚Äôest **pyspark** <br />\n",
    "et son principe de fonctionnement, nous vous conseillons de lire <br />\n",
    "cet article : [PySpark : Tout savoir sur la librairie Python](https://datascientest.com/pyspark)\n",
    "\n",
    "<u>Le d√©but de l‚Äôarticle nous dit ceci </u>:<br />\n",
    "¬´ *Lorsque l‚Äôon parle de traitement de bases de donn√©es sur python, <br />\n",
    "on pense imm√©diatement √† la librairie pandas. Cependant, lorsqu‚Äôon a <br />\n",
    "affaire √† des bases de donn√©es trop massives, les calculs deviennent trop lents.<br />\n",
    "Heureusement, il existe une autre librairie python, assez proche <br />\n",
    "de pandas, qui permet de traiter des tr√®s grandes quantit√©s de donn√©es : PySpark.<br />\n",
    "Apache Spark est un framework open-source d√©velopp√© par l‚ÄôAMPLab <br />\n",
    "de UC Berkeley permettant de traiter des bases de donn√©es massives <br />\n",
    "en utilisant le calcul distribu√©, technique qui consiste √† exploiter <br />\n",
    "plusieurs unit√©s de calcul r√©parties en clusters au profit d‚Äôun seul <br />\n",
    "projet afin de diviser le temps d‚Äôex√©cution d‚Äôune requ√™te.<br />\n",
    "Spark a √©t√© d√©velopp√© en Scala et est au meilleur de ses capacit√©s <br />\n",
    "dans son langage natif. Cependant, la librairie PySpark propose de <br />\n",
    "l‚Äôutiliser avec le langage Python, en gardant des performances <br />\n",
    "similaires √† des impl√©mentations en Scala.<br />\n",
    "Pyspark est donc une bonne alternative √† la librairie pandas lorsqu‚Äôon <br />\n",
    "cherche √† traiter des jeux de donn√©es trop volumineux qui entra√Ænent <br />\n",
    "des calculs trop chronophages.* ¬ª\n",
    "\n",
    "Comme nous le constatons, **pySpark** est un moyen de communiquer <br />\n",
    "avec **Spark** via le langage **Python**.<br />\n",
    "**Spark**, quant √† lui, est un outil qui permet de g√©rer et de coordonner <br />\n",
    "l'ex√©cution de t√¢ches sur des donn√©es √† travers un groupe d'ordinateurs. <br />\n",
    "<u>Spark (ou Apache Spark) est un framework open source de calcul distribu√© <br />\n",
    "in-memory pour le traitement et l'analyse de donn√©es massives</u>.\n",
    "\n",
    "Un autre [article tr√®s int√©ressant et beaucoup plus complet pour <br />\n",
    "comprendre le **fonctionnement de Spark**](https://www.veonum.com/apache-spark-pour-les-nuls/), ainsi que le r√¥le <br />\n",
    "des **Spark Session** que nous utiliserons dans ce projet.\n",
    "\n",
    "<u>Voici √©galement un extrait</u>:\n",
    "\n",
    "*Les applications Spark se composent d‚Äôun pilote (¬´‚ÄØdriver process‚ÄØ¬ª) <br />\n",
    "et de plusieurs ex√©cuteurs (¬´‚ÄØexecutor processes‚ÄØ¬ª). Il peut √™tre configur√© <br />\n",
    "pour √™tre lui-m√™me l‚Äôex√©cuteur (local mode) ou en utiliser autant que <br />\n",
    "n√©cessaire pour traiter l‚Äôapplication, Spark prenant en charge la mise <br />\n",
    "√† l‚Äô√©chelle automatique par une configuration d‚Äôun nombre minimum <br />\n",
    "et maximum d‚Äôex√©cuteurs.*\n",
    "\n",
    "![Sch√©ma de Spark](img/spark-schema.png)\n",
    "\n",
    "*Le driver (parfois appel√© ¬´‚ÄØSpark Session‚ÄØ¬ª) distribue et planifie <br />\n",
    "les t√¢ches entre les diff√©rents ex√©cuteurs qui les ex√©cutent et permettent <br />\n",
    "un traitement r√©parti. Il est le responsable de l‚Äôex√©cution du code <br />\n",
    "sur les diff√©rentes machines.\n",
    "\n",
    "Chaque ex√©cuteur est un processus Java Virtual Machine (JVM) distinct <br />\n",
    "dont il est possible de configurer le nombre de CPU et la quantit√© de <br />\n",
    "m√©moire qui lui est allou√©. <br />\n",
    "Une seule t√¢che peut traiter un fractionnement de donn√©es √† la fois.*\n",
    "\n",
    "Dans les deux environnements (Local et Cloud) nous utiliserons donc **Spark** <br />\n",
    "et nous l‚Äôexploiterons √† travers des scripts python gr√¢ce √† **PySpark**.\n",
    "\n",
    "Dans la <u>version locale</u> de notre script nous **simulerons <br />\n",
    "le calcul distribu√©** afin de valider que notre solution fonctionne.<br />\n",
    "Dans la <u>version cloud</u> nous **r√©aliserons les op√©rations sur un cluster de machine**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364c9f9",
   "metadata": {},
   "source": [
    "## 2.2 Transfert Learning\n",
    "\n",
    "L'√©nonc√© du projet nous demande √©galement de <br />\n",
    "r√©aliser une premi√®re cha√Æne de traitement <br />\n",
    "des donn√©es qui comprendra le preprocessing et <br />\n",
    "une √©tape de r√©duction de dimension.\n",
    "\n",
    "Il est √©galement pr√©cis√© qu'il n'est pas n√©cessaire <br />\n",
    "d'entra√Æner un mod√®le pour le moment.\n",
    "\n",
    "Nous d√©cidons de partir sur une solution de **transfert learning**.\n",
    "\n",
    "Simplement, le **transfert learning** consiste <br />\n",
    "√† utiliser la connaissance d√©j√† acquise <br />\n",
    "par un mod√®le entra√Æn√© (ici **MobileNetV2**) pour <br />\n",
    "l'adapter √† notre probl√©matique.\n",
    "\n",
    "Nous allons fournir au mod√®le nos images, et nous allons <br />\n",
    "<u>r√©cup√©rer l'avant derni√®re couche</u> du mod√®le.<br />\n",
    "En effet la derni√®re couche de mod√®le est une couche softmax <br />\n",
    "qui permet la classification des images ce que nous ne <br />\n",
    "souhaitons pas dans ce projet.\n",
    "\n",
    "L'avant derni√®re couche correspond √† un **vecteur <br />\n",
    "r√©duit** de dimension (1,1,1280).\n",
    "\n",
    "Cela permettra de r√©aliser une premi√®re version du moteur <br />\n",
    "pour la classification des images des fruits.\n",
    "\n",
    "**MobileNetV2** a √©t√© retenu pour sa <u>rapidit√© d'ex√©cution</u>, <br />\n",
    "particuli√®rement adapt√©e pour le traitement d'un gros volume <br />\n",
    "de donn√©es ainsi que la <u>faible dimensionnalit√© du vecteur <br />\n",
    "de caract√©ristique en sortie</u> (1,1,1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89a2da",
   "metadata": {},
   "source": [
    "# 3. D√©ploiement de la solution en local\n",
    "\n",
    "\n",
    "## 3.1 Environnement de travail\n",
    "\n",
    "Pour des raisons de simplicit√©, nous d√©veloppons dans un environnement <br />\n",
    "Linux Unbuntu (ex√©cut√© depuis une machine Windows dans une machine virtuelle)\n",
    "* Pour installer une machine virtuelle :  https://www.malekal.com/meilleurs-logiciels-de-machine-virtuelle-gratuits-ou-payants/\n",
    "\n",
    "## 3.2 Installation de Spark\n",
    "\n",
    "[La premi√®re √©tape consiste √† installer Spark ](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)\n",
    "\n",
    "## 3.3 Installation des packages\n",
    "\n",
    "<u>On installe ensuite √† l'aide de la commande **pip** <br />\n",
    "les packages qui nous seront n√©cessaires</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d9256",
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install Pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a43845",
   "metadata": {},
   "source": [
    "## 3.4 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0c74f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ff67c",
   "metadata": {},
   "source": [
    "## 3.5 D√©finition des PATH pour charger les images <br /> et enregistrer les r√©sultats\n",
    "\n",
    "Dans cette version locale nous partons du principe que les donn√©es <br />\n",
    "sont stock√©es dans le m√™me r√©pertoire que le notebook.<br />\n",
    "Nous n'utilisons qu'un extrait de **300 images** √† traiter dans cette <br />\n",
    "premi√®re version en local.<br />\n",
    "L'extrait des images √† charger est stock√©e dans le dossier **Test1**.<br />\n",
    "Nous enregistrerons le r√©sultat de notre traitement <br />\n",
    "dans le dossier \"**Results_Local**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0aa67",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = PATH+'/data/Test1'\n",
    "PATH_Result = PATH+'/data/Results'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e637a",
   "metadata": {},
   "source": [
    "## 3.6 Cr√©ation de la SparkSession\n",
    "\n",
    "L‚Äôapplication Spark est contr√¥l√©e gr√¢ce √† un processus de pilotage (driver process) appel√© **SparkSession**. <br />\n",
    "<u>Une instance de **SparkSession** est la fa√ßon dont Spark ex√©cute les fonctions d√©finies par l‚Äôutilisateur <br />\n",
    "dans l‚Äôensemble du cluster</u>. <u>Une SparkSession correspond toujours √† une application Spark</u>.\n",
    "\n",
    "<u>Ici nous cr√©ons une session spark en sp√©cifiant dans l'ordre</u> :\n",
    " 1. un **nom pour l'application**, qui sera affich√©e dans l'interface utilisateur Web Spark \"**P8**\"\n",
    " 2. que l'application doit s'ex√©cuter **localement**. <br />\n",
    "   Nous ne d√©finissons pas le nombre de c≈ìurs √† utiliser (comme .master('local[4]) pour 4 c≈ìurs √† utiliser), <br />\n",
    "   nous utiliserons donc tous les c≈ìurs disponibles dans notre processeur.<br />\n",
    " 3. une option de configuration suppl√©mentaire permettant d'utiliser le **format \"parquet\"** <br />\n",
    "   que nous utiliserons pour enregistrer et charger le r√©sultat de notre travail.\n",
    " 4. vouloir **obtenir une session spark** existante ou si aucune n'existe, en cr√©er une nouvelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bea157",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('P8')\n",
    "             .master('local')\n",
    "             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "             .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b53ac",
   "metadata": {},
   "source": [
    "<u>Nous cr√©ons √©galement la variable \"**sc**\" qui est un **SparkContext** issue de la variable **spark**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aeccb1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a086010",
   "metadata": {},
   "source": [
    "<u>Affichage des informations de Spark en cours d'execution</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bf13b",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a88b0",
   "metadata": {},
   "source": [
    "## 3.7 Traitement des donn√©es\n",
    "\n",
    "<u>Dans la suite de notre flux de travail, <br />\n",
    "nous allons successivement</u> :\n",
    "1. Pr√©parer nos donn√©es\n",
    "    1. Importer les images dans un dataframe **pandas UDF**\n",
    "    2. Associer aux images leur **label**\n",
    "    3. Pr√©processer en **redimensionnant nos images pour <br />\n",
    "       qu'elles soient compatibles avec notre mod√®le**\n",
    "2. Pr√©parer notre mod√®le\n",
    "    1. Importer le mod√®le **MobileNetV2**\n",
    "    2. Cr√©er un **nouveau mod√®le** d√©pourvu de la derni√®re couche de MobileNetV2\n",
    "3. D√©finir le processus de chargement des images et l'application <br />\n",
    "   de leur featurisation √† travers l'utilisation de pandas UDF\n",
    "3. Ex√©cuter les actions d'extraction de features\n",
    "4. Enregistrer le r√©sultat de nos actions\n",
    "5. Tester le bon fonctionnement en chargeant les donn√©es enregistr√©es\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fe0bc",
   "metadata": {},
   "source": [
    "### 3.7.1 Chargement des donn√©es\n",
    "\n",
    "Les images sont charg√©es au format binaire, ce qui offre, <br />\n",
    "plus de souplesse dans la fa√ßon de pr√©traiter les images.\n",
    "\n",
    "Avant de charger les images, nous sp√©cifions que nous voulons charger <br />\n",
    "uniquement les fichiers dont l'extension est **jpg**.\n",
    "\n",
    "Nous indiquons √©galement de charger tous les objets possibles contenus <br />\n",
    "dans les sous-dossiers du dossier communiqu√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e53b9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645faeaf",
   "metadata": {},
   "source": [
    "<u>Affichage des 5 premi√®res images contenant</u> :\n",
    " - le path de l'image\n",
    " - la date et heure de sa derni√®re modification\n",
    " - sa longueur\n",
    " - son contenu encod√© en valeur hexad√©cimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863981e5",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b0494",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d47705",
   "metadata": {},
   "source": [
    "### 3.7.2 Pr√©paration du mod√®le\n",
    "\n",
    "Je vais utiliser la technique du **transfert learning** pour extraire les features des images.<br />\n",
    "J'ai choisi d'utiliser le mod√®le **MobileNetV2** pour sa rapidit√© d'ex√©cution compar√©e <br />\n",
    "√† d'autres mod√®les comme *VGG16* par exemple.\n",
    "\n",
    "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, <br />\n",
    "je vous invite √† lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
    "\n",
    "<u>Voici le sch√©ma de son architecture globale</u> : \n",
    "\n",
    "![Architecture de MobileNetV2](img/mobilenetv2_architecture.png)\n",
    "\n",
    "Il existe une derni√®re couche qui sert √† classer les images <br />\n",
    "selon 1000 cat√©gories que nous ne voulons pas utiliser.<br />\n",
    "L'id√©e dans ce projet est de r√©cup√©rer le **vecteur de caract√©ristiques <br />\n",
    "de dimensions (1,1,1280)** qui servira, plus tard, au travers d'un moteur <br />\n",
    "de classification √† reconnaitre les diff√©rents fruits du jeu de donn√©es.\n",
    "\n",
    "Comme d'autres mod√®les similaires, **MobileNetV2**, lorsqu'on l'utilise <br />\n",
    "en incluant toutes ses couches, attend obligatoirement des images <br />\n",
    "de dimension (224,224,3). Nos images √©tant toutes de dimension (100,100,3), <br />\n",
    "nous devrons simplement les **redimensionner** avant de les confier au mod√®le.\n",
    "\n",
    "<u>Dans l'odre</u> :\n",
    " 1. Nous chargeons le mod√®le **MobileNetV2** avec les poids **pr√©calcul√©s** <br />\n",
    "    issus d'**imagenet** et en sp√©cifiant le format de nos images en entr√©e\n",
    " 2. Nous cr√©ons un nouveau mod√®le avec:\n",
    "  - <u>en entr√©e</u> : l'entr√©e du mod√®le MobileNetV2\n",
    "  - <u>en sortie</u> : l'avant derni√®re couche du mod√®le MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd9bdf",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6b68d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b197379",
   "metadata": {},
   "source": [
    "Affichage du r√©sum√© de notre nouveau mod√®le o√π nous constatons <br />\n",
    "que <u>nous r√©cup√©rons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8207725",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0adcf5",
   "metadata": {},
   "source": [
    "Tous les workeurs doivent pouvoir acc√©der au mod√®le ainsi qu'√† ses poids. <br />\n",
    "Une bonne pratique consiste √† charger le mod√®le sur le driver puis √† diffuser <br />\n",
    "ensuite les poids aux diff√©rents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc53ff0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0e34e",
   "metadata": {},
   "source": [
    "<u>Mettons cela sous forme de fonction</u> :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7d54c",
   "metadata": {},
   "source": [
    "! **Modification du code pour ne pas t√©l√©charger inutilement les poids du mod√®le** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd51ba9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    # Modificatifation pour ne pas t√©l√©charger les poids du mod√®le weights='imagenet'\n",
    "    model = MobileNetV2(weights=None,\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5620876",
   "metadata": {},
   "source": [
    "### 3.7.3 D√©finition du processus de chargement des images et application <br/>de leur featurisation √† travers l'utilisation de pandas UDF\n",
    "\n",
    "Ce notebook d√©finit la logique par √©tapes, jusqu'√† Pandas UDF.\n",
    "\n",
    "<u>L'empilement des appels est la suivante</u> :\n",
    "\n",
    "- Pandas UDF\n",
    "  - featuriser une s√©rie d'images pd.Series\n",
    "   - pr√©traiter une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e5f69",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf2ef9",
   "metadata": {},
   "source": [
    "### 3.7.4 Ex√©cution des actions d'extraction de features\n",
    "\n",
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de tr√®s grandes images), <br />\n",
    "peuvent rencontrer des erreurs de type Out Of Memory (OOM).<br />\n",
    "Si vous rencontrez de telles erreurs dans la cellule ci-dessous, <br />\n",
    "essayez de r√©duire la taille du lot Arrow via 'maxRecordsPerBatch'\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet <br />\n",
    "et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30d28c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8f95d",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant ex√©cuter la featurisation sur l'ensemble de notre DataFrame Spark.<br />\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout d√©pend du volume de donn√©es √† traiter. <br />\n",
    "\n",
    "Notre jeu de donn√©es de **Test** contient **22819 images**. <br />\n",
    "Cependant, dans l'ex√©cution en mode **local**, <br />\n",
    "nous <u>traiterons un ensemble r√©duit de **330 images**</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1767c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features_df = images.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e83ec",
   "metadata": {},
   "source": [
    "<u>Rappel du PATH o√π seront inscrits les fichiers au format \"**parquet**\" <br />\n",
    "contenant nos r√©sultats, √† savoir, un DataFrame contenant 3 colonnes</u> :\n",
    " 1. Path des images\n",
    " 2. Label de l'image\n",
    " 3. Vecteur de caract√©ristiques de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcdb0f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8901db3",
   "metadata": {},
   "source": [
    "<u>Enregistrement des donn√©es trait√©es au format \"**parquet**\"</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d07466",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd0c67",
   "metadata": {},
   "source": [
    "## 3.7.5 Reduction Dimension PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9399de",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Recherche                                                                                              du nombre optimal de composantes principales k pour PCA\n",
    "k_values = [150, 200, 250, 300, 350, 400]\n",
    "\n",
    "for k in k_values:\n",
    "    pca = PCA(k=k, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
    "    model = pca.fit(scaled_df)\n",
    "    var = sum(model.explainedVariance)\n",
    "    print(f\"k={k} ‚Üí {var:.1%}\")\n",
    "    if var >= 0.90:\n",
    "        print(f\"‚úÖ k={k} atteint 90% !\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecebf43",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import n√©cessaire pour la PCA\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Conversion des array en vecteur\n",
    "# PySpark ML n√©cessite que les features soient au format Vector.\n",
    "# Nous devons donc convertir nos arrays Python en Vectors Spark.\n",
    "def array_to_vector(arr):\n",
    "    \"\"\"Convertit un array Python en Vector Spark.\"\"\"\n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "# Cr√©er l'UDF\n",
    "array_to_vector_udf = udf(array_to_vector, VectorUDT())\n",
    "\n",
    "# Appliquer la conversion\n",
    "print(\"Conversion des features en Vectors...\")\n",
    "features_vectors_df = features_df.withColumn(\n",
    "    \"features_vector\", \n",
    "    array_to_vector_udf(col(\"features\"))\n",
    ")\n",
    "\n",
    "print(f\"Conversion termin√©e : {features_vectors_df.count()} images\")\n",
    "features_vectors_df.select(\"path\", \"features_vector\").show(5, truncate=True)\n",
    "\n",
    "# Standardisation des donn√©es\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_vector\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,   # Normaliser l'√©cart-type\n",
    "    withMean=True   # Centrer sur la moyenne\n",
    ")\n",
    "\n",
    "# Entra√Æner le scaler\n",
    "scaler_model = scaler.fit(features_vectors_df)\n",
    "\n",
    "# Transformer les donn√©es\n",
    "scaled_df = scaler_model.transform(features_vectors_df)\n",
    "\n",
    "# Application de la PCA\n",
    "\"\"\"\n",
    "R√©duction de 1280 dimensions √† 50 dimensions.\n",
    "Le nombre k=50 est un compromis entre compression et perte d'information.\n",
    "\"\"\"\n",
    "print(\" Application de la PCA (1280 ‚Üí 50 dimensions)...\")\n",
    "\n",
    "# Configuration de la PCA\n",
    "k_components = 350  # Nombre de composantes √† garder\n",
    "pca = PCA(\n",
    "    k=k_components,\n",
    "    inputCol=\"scaled_features\",\n",
    "    outputCol=\"pca_features\"\n",
    ")\n",
    "\n",
    "# Entra√Æner la PCA\n",
    "print(f\"‚è≥ Entra√Ænement de la PCA avec k={k_components}...\")\n",
    "pca_model = pca.fit(scaled_df)\n",
    "\n",
    "# Transformer les donn√©es\n",
    "pca_df = pca_model.transform(scaled_df)\n",
    "\n",
    "print(\"PCA termin√©e\")\n",
    "\n",
    "# Analyse de la variance expliqu√©e\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSE DE LA VARIANCE EXPLIQU√âE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# R√©cup√©rer la variance expliqu√©e par chaque composante\n",
    "explained_variance = pca_model.explainedVariance\n",
    "\n",
    "# Calculer la variance cumul√©e\n",
    "cumulative_variance = 0.0\n",
    "print(\"\\n Variance expliqu√©e par composante :\")\n",
    "print(f\"{'Composante':<15} {'Variance':<15} {'Cumulative':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(min(10, k_components)):  # Afficher les 10 premi√®res\n",
    "    cumulative_variance += explained_variance[i]\n",
    "    print(f\"PC{i+1:<13} {explained_variance[i]:<15.4f} {cumulative_variance:<15.4f}\")\n",
    "\n",
    "# Variance totale\n",
    "total_variance = sum(explained_variance)\n",
    "print(\"-\" * 45)\n",
    "print(f\"\\n Variance totale expliqu√©e par les {k_components} composantes : {total_variance:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878ef7e",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARAISON : AVANT vs APR√àS PCA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  FEATURES ORIGINALES (MobileNetV2)                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  ‚Ä¢ Dimensions : 1280                                ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Taille par image : ~5 KB                         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Taille totale : {features_df.count() * 5 / 1024:.2f} MB                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  FEATURES APR√àS PCA                                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  ‚Ä¢ Dimensions : {k_components}                      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Taille par image : ~0.2 KB                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Taille totale : {features_df.count() * 0.2 / 1024:.2f} MB     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Variance conserv√©e : {total_variance:.2%}        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "üìâ R√âDUCTION DE TAILLE : {1280/k_components:.1f}x plus compact\n",
    "üí∞ √âCONOMIES S3 : {(1 - k_components/1280) * 100:.1f}% de stockage en moins\n",
    "‚ö° VITESSE : {1280/k_components:.1f}x plus rapide pour les traitements futurs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b07e95",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n √âchantillon de r√©sultats :\\n\")\n",
    "pca_df.select(\n",
    "    \"path\",\n",
    "    col(\"features_vector\").alias(\"features_1280d\"),\n",
    "    col(\"pca_features\").alias(\"features_50d\")\n",
    ").show(5, truncate=True)\n",
    "\n",
    "print(\"\\n  Sauvegarde des r√©sultats PCA...\")\n",
    "\n",
    "# Extraire le label depuis le path\n",
    "pca_df = pca_df.withColumn(\n",
    "    \"label\",\n",
    "    element_at(split(col(\"path\"), \"/\"), -2)\n",
    ")\n",
    "\n",
    "# S√©lectionner les colonnes finales\n",
    "final_df = pca_df.select(\n",
    "    \"path\",\n",
    "    \"label\",\n",
    "    \"pca_features\"\n",
    ")\n",
    "\n",
    "# Sauvegarder au format Parquet (recommand√©)\n",
    "output_path_parquet = PATH_Result + \"/pca_results.parquet\"\n",
    "print(f\" Sauvegarde Parquet : {output_path_parquet}\")\n",
    "\n",
    "final_df.write.mode(\"overwrite\").parquet(output_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9506f21",
   "metadata": {},
   "source": [
    "## 3.8 Chargement des donn√©es enregistr√©es et validation du r√©sultat\n",
    "\n",
    "<u>On charge les donn√©es fraichement enregistr√©es dans un **DataFrame Pandas**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19243bf5",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f15070",
   "metadata": {},
   "source": [
    "<u>On affiche les 5 premi√®res lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bcdeb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2794fca",
   "metadata": {},
   "source": [
    "<u>On valide que la dimension du vecteur de caract√©ristiques des images est bien de dimension 1280</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb933b9",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5348d",
   "metadata": {},
   "source": [
    "Nous venons de valider le processus sur un jeu de donn√©es all√©g√© en local <br />\n",
    "o√π nous avons simul√© un cluster de machines en r√©partissant la charge de travail <br />\n",
    "sur diff√©rents c≈ìurs de processeur au sein d'une m√™me machine.\n",
    "\n",
    "Nous allons maintenant g√©n√©raliser le processus en d√©ployant notre solution <br />\n",
    "sur un r√©el cluster de machines et nous travaillerons d√©sormais sur la totalit√© <br />\n",
    "des 22819 images de notre dossier \"Test\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0f7c1",
   "metadata": {},
   "source": [
    "# 4. D√©ploiement de la solution sur le cloud\n",
    "\n",
    "Maintenant que nous avons v√©rifi√© que notre solution fonctionne, <br />\n",
    "il est temps de la <u>d√©ployer √† plus grande √©chelle sur un vrai cluster de machines</u>.\n",
    "\n",
    "**Attention**, *je travaille sous Linux avec une version Ubuntu, <br />\n",
    "les commandes d√©crites ci-dessous sont donc r√©alis√©es <br />\n",
    "exclusivement dans cet environnement.*\n",
    "\n",
    "<u>Plusieurs contraintes se posent</u> :\n",
    " 1. Quel prestataire de Cloud choisir ?\n",
    " 2. Quelles solutions de ce prestataire adopter ?\n",
    " 3. O√π stocker nos donn√©es ?\n",
    " 4. Comment configurer nos outils dans ce nouvel environnement ?\n",
    " \n",
    "## 4.1 Choix du prestataire cloud : AWS\n",
    "\n",
    "Le prestataire le plus connu et qui offre √† ce jour l'offre <br />\n",
    "la plus large dans le cloud computing est **Amazon Web Services** (AWS).<br />\n",
    "Certaines de leurs offres sont parfaitement adapt√©es √† notre probl√©matique <br />\n",
    "et c'est la raison pour laquelle j'utiliserai leurs services.\n",
    "\n",
    "L'objectif premier est de pouvoir, gr√¢ce √† AWS, <u>louer de la puissance de calcul √† la demande</u>. <br />\n",
    "L'id√©e √©tant de pouvoir, quel que soit la charge de travail, <br />\n",
    "obtenir suffisamment de puissance de calcul pour pouvoir traiter nos images, <br />\n",
    "m√™me si le volume de donn√©es venait √† fortement augmenter.\n",
    "\n",
    "De plus, la capacit√© d'utiliser cette puissance de calcul √† la demande <br />\n",
    "permet de diminuer drastiquement les co√ªts si l'on compare les co√ªts d'une location <br />\n",
    "de serveur complet sur une dur√©e fixe (1 mois, 1 ann√©e par exemple).\n",
    "\n",
    "## 4.2 Choix de la solution technique : EMR\n",
    "\n",
    "<u>Plusieurs solutions s'offre √† nous</u> :\n",
    "1. Solution **IAAS** (Infrastructure AS A Service)\n",
    " - Dans cette configuration **AWS** met √† notre disposition des serveurs vierges <br />\n",
    "   sur lequel nous avons un acc√®s en administrateur, ils sont nomm√©s **instance EC2**.<br />\n",
    "   Pour faire simple, nous pouvons avec cette solution reproduire pratiquement <br />\n",
    "   √† l'identique la solution mis en ≈ìuvre en local sur notre machine.<br />\n",
    "   <u>On installe nous-m√™me l'int√©gralit√© des outils puis on ex√©cute notre script</u> :\n",
    "  - Installation de **Spark**, **Java** etc.\n",
    "  - Installation de **Python** (via Anaconda par exemple)\n",
    "  - Installation de **Jupyter Notebook**\n",
    "  - Installation des **librairies compl√©mentaires**\n",
    "  - Il faudra bien √©videment veiller √† **impl√©menter les librairies \n",
    "    n√©cessaires √† toutes les machines (workers) du cluster**\n",
    "  - <u>Avantages</u> :\n",
    "      - Libert√© totale de mise en ≈ìuvre de la solution\n",
    "      - Facilit√© de mise en ≈ìuvre √† partir d'un mod√®le qui s'ex√©cute en local sur une machine Linux\n",
    "  - <u>Inconv√©nients</u> :\n",
    "      - Cronophage\n",
    "          - N√©cessit√© d'installer et de configurer toute la solution\n",
    "      - Possible probl√®mes techniques √† l'installation des outils (des probl√©matiques qui <br />\n",
    "        n'existaient pas en local sur notre machine peuvent apparaitre sur le serveur EC2)\n",
    "      - Solution non p√©renne dans le temps, il faudra veiller √† la mise √† jour des outils <br />\n",
    "        et √©ventuellement devoir r√©installer Spark, Java etc. \n",
    "2. Solution **PAAS** (Plateforme As A Service)\n",
    " - **AWS** fournit √©norm√©ment de services diff√©rents, dans l'un de ceux-l√† <br />\n",
    "   il existe une offre qui permet de louer des **instances EC2** <br />\n",
    "   avec des applications pr√©install√©es et configur√©es : il s'agit du **service EMR**.\n",
    " - **Spark** y sera d√©j√† install√©\n",
    " - Possibilit√© de demander l'installation de **Tensorflow** ainsi que **JupyterHub**\n",
    " - Possibilit√© d'indiquer des **packages compl√©mentaires** √† installer <br />\n",
    "   √† l'initialisation du serveur **sur l'ensemble des machines du cluster**.\n",
    " - <u>Avantages</u> :\n",
    "     - Facilit√© de mise en ≈ìuvre\n",
    "         - Il suffit de tr√®s peu de configuration pour obtenir <br />\n",
    "           un environnement parfaitement fonctionnel\n",
    "     - Rapidit√© de mise en ≈ìuvre\n",
    "         - Une fois la premi√®re configuration r√©alis√©e, il est tr√®s facile <br />\n",
    "           et tr√®s rapide de recr√©er des clusters √† l'identique qui seront <br />\n",
    "           disponibles presque instantan√©ment (le temps d'instancier les <br />\n",
    "           serveurs soit environ 15/20 minutes)\n",
    "     - Solutions mat√©rielless et logicielles optimis√©es par les ing√©nieurs d'AWS\n",
    "         - On sait que les versions install√©es vont fonctionner <br />\n",
    "           et que l'architecture propos√©e est optimis√©e\n",
    "     - Stabilit√© de la solution\n",
    "    - Solution √©volutive\n",
    "        Il est facile d‚Äôobtenir √† chaque nouvelle instanciation une version √† jour <br />\n",
    "        de chaque package, en √©tant garanti de leur compatibilit√© avec le reste de l‚Äôenvironnement.\n",
    "  - Plus s√©curis√©\n",
    "\t- Les √©ventuels patchs de s√©curit√© seront automatiquement mis √† jour <br />\n",
    "      √† chaque nouvelle instanciation du cluster EMR.\n",
    " - <u>Inconv√©nients</u> :\n",
    "     - Peut-√™tre un certain manque de libert√© sur la version des packages disponibles ? <br />\n",
    "       M√™me si je n'ai pas constat√© ce probl√®me.\n",
    "   \n",
    "\n",
    "Je retiens la solution **PAAS** en choisissant d'utiliser <br />\n",
    "le service **EMR** d'Amazon Web Services.<br />\n",
    "Je la trouve plus adapt√©e √† notre probl√©matique et permet <br />\n",
    "une mise en ≈ìuvre qui soit √† la fois plus rapide et <br />\n",
    "plus efficace que la solution IAAS.\n",
    "\n",
    "## 4.3 Choix de la solution de stockage des donn√©es : Amazon S3\n",
    "\n",
    "<u>Amazon propose une solution tr√®s efficace pour la gestion du stockage des donn√©es</u> : **Amazon S3**. <br />\n",
    "S3 pour Amazon Simple Storage Service.\n",
    "\n",
    "Il pourrait √™tre tentant de stocker nos donn√©es sur l'espace allou√© par le serveur **EC2**, <br />\n",
    "mais si nous ne prenons aucune mesure pour les sauvegarder ensuite sur un autre support, <br />\n",
    "<u>les donn√©es seront perdues</u> lorsque le serveur sera r√©sili√© (on r√©silie le serveur lorsqu'on <br />\n",
    "ne s'en sert pas pour des raisons de co√ªt).<br />\n",
    "De fait, si l'on d√©cide d'utiliser l'espace disque du serveur EC2 il faudra imaginer <br />\n",
    "une solution pour sauvegarder les donn√©es avant la r√©siliation du serveur.\n",
    "De plus, nous serions expos√©s √† certaines probl√©matiques si nos donn√©es venaient √† <br />\n",
    "**saturer** l'espace disponible de nos serveurs (ralentissements, disfonctionnements).\n",
    "\n",
    "<u>Utiliser **Amazon S3** permet de s'affranchir de toutes ces probl√©matiques</u>. <br />\n",
    "L'espace disque disponible est **illimit√©**, et il est **ind√©pendant de nos serveurs EC2**. <br />\n",
    "L'acc√®s aux donn√©es est **tr√®s rapide** car nous restons dans l'environnement d'AWS <br />\n",
    "et nous prenons soin de <u>choisir la m√™me r√©gion pour nos serveurs **EC2** et **S3**</u>.\n",
    "\n",
    "De plus, comme nous le verrons <u>il est possible d'acc√©der aux donn√©es sur **S3** <br />\n",
    "    de la m√™me mani√®re que l'on **acc√®de aux donn√©es sur un disque local**</u>.<br />\n",
    "Nous utiliserons simplement un **PATH au format s3://...** .\n",
    "\n",
    "## 4.4 Configuration de l'environnement de travail\n",
    "\n",
    "La premi√®re √©tape est d'installer et de configurer [**AWS Cli**](https://aws.amazon.com/fr/cli/),<br />\n",
    "il s'agit de l'**interface en ligne de commande d'AWS**.<br />\n",
    "Elle nous permet d'**interagir avec les diff√©rents services d'AWS**, comme **S3** par exemple.\n",
    "\n",
    "Pour pouvoir utiliser **AWS Cli**, il faut le configurer en cr√©ant pr√©alablement <br />\n",
    "un utilisateur √† qui on donnera les autorisations dont nous aurons besoin.<br />\n",
    "Dans ce projet il faut que l'utilisateur ait √† minima un contr√¥le total sur le service S3.\n",
    "\n",
    "<u>La gestion des utilisateurs et de leurs droits s'effectue via le service **AMI**</u> d'AWS.\n",
    "\n",
    "Une fois l'utilisateur cr√©√© et ses autorisations configur√©es nous cr√©ons une **paire de cl√©s** <br />\n",
    "qui nous permettra de nous **connecter sans √† avoir √† devoir saisir syst√©matiquement notre login/mot de passe**.<br />\n",
    "\n",
    "Il faut √©galement configurer l'**acc√®s SSH** √† nos futurs serveurs EC2. <br />\n",
    "Ici aussi, via un syst√®me de cl√©s qui nous dispense de devoir nous authentifier \"√† la main\" √† chaque connexion.\n",
    "\n",
    "Toutes ses √©tapes de configuration sont parfaitement d√©crites <br />\n",
    "dans le cours du projet: [D√©couvrez le cloud avec Amazon Web Services / Faites vos premiers pas sur AWS](https://openclassrooms.com/fr/courses/4810836-decouvrez-le-cloud-avec-amazon-web-services/7821712-faites-vos-premiers-pas-sur-aws)\n",
    "\n",
    "## 4.5 Upload de nos donn√©es sur S3\n",
    "\n",
    "Nos outils sont configur√©s. <br />\n",
    "Il faut maintenant uploader nos donn√©es de travail sur Amazon S3.\n",
    "\n",
    "Ici aussi les √©tapes sont d√©crites avec pr√©cision <br />\n",
    "dans le cours [D√©couvrez le cloud avec Amazon Web Services / Stockez et acc√©dez √† des fichiers sur Amazon S3](https://openclassrooms.com/fr/courses/4810836-decouvrez-le-cloud-avec-amazon-web-services/7822690-stockez-et-accedez-a-des-fichiers-sur-amazon-s3)\n",
    "\n",
    "Je d√©cide de n'uploader que les donn√©es contenues dans le dossier **Test** du [jeu de donn√©es du projet](https://www.kaggle.com/moltean/fruits/download)\n",
    "\n",
    "\n",
    "La premi√®re √©tape consiste √† **cr√©er un bucket sur S3** <br />\n",
    "dans lequel nous uploaderons les donn√©es du projet:\n",
    "- **aws s3 mb s3://p8-data**\n",
    "\n",
    "On v√©rifie que le bucket √† bien √©t√© cr√©√©\n",
    "- **aws s3 ls**\n",
    " - Si le nom du bucket s'affiche alors c'est qu'il a √©t√© correctement cr√©√©.\n",
    "\n",
    "On copie ensuite le contenu du dossier \"**Test**\" <br />\n",
    "dans un r√©pertoire \"**Test**\" sur notre bucket \"**p8-data**\":\n",
    "1. On se place √† l'int√©rieur du r√©pertoire **Test**\n",
    "2. **aws sync . s3://p8-data/Test**\n",
    "\n",
    "La commande **sync** est utile pour synchroniser deux r√©pertoires.\n",
    "\n",
    "<u>Nos donn√©es du projet sont maintenant disponibles sur Amazon S3</u>.\n",
    "\n",
    "## 4.6 Configuration du serveur EMR\n",
    "\n",
    "Une fois encore, le cours [D√©couvrez le cloud avec Amazon Web Services / D√©couvrez les services d'Amazon EC2](https://openclassrooms.com/fr/courses/4810836-decouvrez-le-cloud-avec-amazon-web-services/7822091-demarrez-votre-premiere-instance-ec2) <br /> d√©taille l'essentiel des √©tapes pour lancer un cluster avec **EMR**.\n",
    "\n",
    "<u>Je d√©taillerai ici les √©tapes particuli√®res qui nous permettent <br />\n",
    "de configurer le serveur selon nos besoins</u> :\n",
    "\n",
    "1. Cliquez sur Cr√©er un cluster\n",
    "![Cr√©er un cluster](img/EMR_creer.png)\n",
    "2. Cliquez sur Acc√©der aux options avanc√©es\n",
    "![Cr√©er un cluster](img/EMR_options_avancees.png)\n",
    "\n",
    "### 4.6.1 √âtape 1 : Logiciels et √©tapes\n",
    "\n",
    "#### 4.6.1.1 Configuration des logiciels\n",
    "\n",
    "<u>S√©lectionnez les packages dont nous aurons besoin comme dans la capture d'√©cran</u> :\n",
    "1. Nous s√©lectionnons la derni√®re version d'**EMR**, soit la version **6.3.0** au moment o√π je r√©dige ce document\n",
    "2. Nous cochons bien √©videment **Hadoop** et **Spark** qui seront pr√©install√©s dans leur version la plus r√©cente\n",
    "3. Nous aurons √©galement besoin de **TensorFlow** pour importer notre mod√®le et r√©aliser le **transfert learning**\n",
    "4. Nous travaillerons enfin avec un **notebook Jupyter** via l'application **JupyterHub**<br />\n",
    " - Comme nous le verrons dans un instant nous allons <u>param√©trer l'application afin que les notebooks</u>, <br />\n",
    "   comme le reste de nos donn√©es de travail, <u>soient enregistr√©s directement sur S3</u>.\n",
    "![Cr√©er un cluster](img/EMR_configuration_logiciels.png)\n",
    "\n",
    "#### 4.6.1.2 Modifier les param√®tres du logiciel\n",
    "\n",
    "<u>Param√©trez la persistance des notebooks cr√©√©s et ouvert via JupyterHub</u> :\n",
    "- On peut √† cette √©tape effectuer des demandes de param√©trage particuli√®res sur nos applications. <br />\n",
    "  L'objectif est, comme pour le reste de nos donn√©es de travail, <br />\n",
    "  d'√©viter toutes les probl√©matiques √©voqu√©es pr√©c√©demment. <br />\n",
    "  C'est l'objectif √† cette √©tape, <u>nous allons enregistrer <br />\n",
    "  et ouvrir les notebooks</u> non pas sur l'espace disque de  l'instance EC2 (comme <br />\n",
    "  ce serait le cas dans la configuration par d√©faut de JupyterHub) mais <br />\n",
    "  <u>directement sur **Amazon S3**</u>.\n",
    "- <u>deux solutions sont possibles pour r√©aliser cela</u> :\n",
    " 1. Cr√©er un **fichier de configuration JSON** que l'on **upload sur S3** et on indique ensuite le chemin d‚Äôacc√®s au fichier JSON\n",
    " 2. Rentrez directement la configuration au format JSON\n",
    " \n",
    "J'ai personnellement cr√©√© un fichier JSON lors de la cr√©ation de ma premi√®re instance EMR, <br />\n",
    "puis lorsqu'on d√©cide de cloner notre serveur pour en recr√©er un facilement √† l'identique, <br />\n",
    "la configuration du fichier JSON se retrouve directement copi√© comme dans la capture ci-dessous.\n",
    "\n",
    "<u>Voici le contenu de mon fichier JSON</u> :  [{\"classification\":\"jupyter-s3-conf\",\"properties\":{\"s3.persistence.bucket\":\"p8-data\",\"s3.persistence.enabled\":\"true\"}}]\n",
    " Appuyez ensuite sur \"**Suivant**\"\n",
    "![Modifier les param√®tres du logiciel](img/EMR_parametres_logiciel.png)\n",
    "\n",
    "### 4.6.2 √âtape 2 : Mat√©riel\n",
    "\n",
    "A cette √©tape, laissez les choix par d√©faut. <br />\n",
    "<u>L'important ici est la s√©lection de nos instances</u> :\n",
    "\n",
    "1. je choisi les instances de type **M5** qui sont des **instances de type √©quilibr√©s**\n",
    "2. je choisi le type **xlarge** qui est l'instance la **moins on√©reuse disponible**\n",
    " [Plus d'informations sur les instances M5 Amazon EC2](https://aws.amazon.com/fr/ec2/instance-types/m5/)\n",
    "3. Je s√©lectionne **1 instance Ma√Ætre** (le driver) et **2 instances Principales** (les workeurs) <br />\n",
    "   soit **un total de 3 instance EC2**.\n",
    "![Choix du materiel](img/EMR_materiel.png)\n",
    "\n",
    "### 4.6.3 √âtape 3 : Param√®tres de cluster g√©n√©raux\n",
    "\n",
    "#### 4.6.3.1 Options g√©n√©rales\n",
    "<u>La premi√®re chose √† faire est de donner un nom au cluster</u> :<br />\n",
    "*J'ai √©galement d√©coch√© \"Protection de la r√©siliation\" pour des raisons pratiques.*\n",
    "    \n",
    "![Nom du Cluster](img/EMR_nom_cluster.png)\n",
    "\n",
    "#### 4.6.3.2 Actions d'amor√ßage\n",
    "\n",
    "Nous allons √† cette √©tape **choisir les packages manquants √† installer** et qui <br />\n",
    "nous serons utiles dans l'ex√©cution de notre notebook.<br />\n",
    "<u>L'avantage de r√©aliser cette √©tape maintenant est que les packages <br />\n",
    "install√©s le seront sur l'ensemble des machines du cluster</u>.\n",
    "\n",
    "Nous cr√©ons un fichier nomm√© \"**bootstrap-emr.sh**\" que nous <u>uploadons <br />\n",
    "sur S3</u>(je l‚Äôinstalle √† la racine de mon **bucket \"p8-data\"**) et nous l'ajoutons <br />\n",
    "comme indiqu√© dans la capture d'√©cran ci-dessous:\n",
    "![Actions d'amorcage](img/EMR_amorcage.png)\n",
    "\n",
    "Voici le contenu du fichier **bootstrap-emr.sh**<br />\n",
    "Comme on peut le constater il s'agit simplement de commande \"**pip install**\" <br />\n",
    "pour **installer les biblioth√®ques manquantes** comme r√©alis√© en local.<br />\n",
    "Une fois encore, <u>il est n√©cessaire de r√©aliser ces actions √† cette √©tape</u> <br />\n",
    "pour que <u>les packages soient install√©s sur l'ensemble des machines du cluster</u> <br />\n",
    "et non pas uniquement sur le driver, comme cela serait le cas si nous ex√©cutions <br />\n",
    "ces commandes directement dans le notebook JupyterHub ou dans la console EMR (connect√© au driver).\n",
    "![Contenu du fichier bootstrap](img/EMR_bootstrap.png)\n",
    "\n",
    "**setuptools** et **pip** sont mis √† jour pour √©viter une probl√©matique <br />\n",
    "avec l'installation du package **pyarrow**.<br />\n",
    "**Pandas** a eu droit √† une mise √† jour majeur (1.3.0) il y a moins d'une semaine <br />\n",
    "au moment de la r√©daction de ce notebook, et la nouvelle version de **Pandas** <br />\n",
    "n√©cessite une version plus r√©cente de **Numpy** que la version install√©e par <br />\n",
    "d√©faut (1.16.5) √† l'initialisation des instances **EC2**. <u>Il ne semble pas <br />\n",
    "possible d'imposer une autre version de Numpy que celle install√© par <br />\n",
    "d√©faut</u> m√™me si on force l'installation d'une version r√©cente de **Numpy** <br />\n",
    "(en tout cas, ni simplement ni intuitivement).<br />\n",
    "La mise √† jour √©tant tr√®s r√©cente <u>la version de **Numpy** n'est pas encore <br />\n",
    "mise √† jour sur **EC2**</u> mais on peut imaginer que ce sera le cas tr√®s rapidement <br />\n",
    "et il ne sera plus n√©cessaire d'imposer une version sp√©cifique de **Pandas**.<br />\n",
    "En attendant, je demande <u>l'installation de l'avant derni√®re version de **Pandas (1.2.5)**</u>\n",
    "\n",
    "On clique ensuite sur ***Suivant***\n",
    "\n",
    "### 4.6.4 √âtape 4 : S√©curit√©\n",
    "\n",
    "#### 4.6.4.1 Options de s√©curit√©\n",
    "\n",
    "A cette √©tape nous s√©lectionnons la **paire de cl√©s EC2** cr√©√© pr√©c√©demment. <br />\n",
    "Elle nous permettra de se connecter en **ssh** √† nos **instances EC2** <br />\n",
    "sans avoir √† entrer nos login/mot de passe.<br />\n",
    "On laisse les autres param√®tres par d√©faut. <br />\n",
    "Et enfin, on clique sur \"***Cr√©er un cluster***\"\n",
    " \n",
    "![EMR S√©curit√©](img/EMR_securite.png)\n",
    "\n",
    "## 4.7 Instanciation du serveur\n",
    "\n",
    "Il ne nous reste plus qu'√† attendre que le serveur soit pr√™t. <br />\n",
    "Cette √©tape peut prendre entre **15 et 20 minutes**.\n",
    "\n",
    "<u>Plusieurs √©tapes s'encha√Æne, on peut suivre l'avanc√© du statut du **cluster EMR**</u> :\n",
    "\n",
    "![Instanciation √©tape 1](img/EMR_instanciation_01.png)\n",
    "![Instanciation √©tape 2](img/EMR_instanciation_02.png)\n",
    "![Instanciation √©tape 3](img/EMR_instanciation_03.png)\n",
    "\n",
    "<u>Lorsque le statut affiche en vert: \"**En attente**\" cela signifie que l'instanciation <br />\n",
    "s'est bien d√©roul√©e et que notre serveur est pr√™t √† √™tre utilis√©</u>. \n",
    "\n",
    "## 4.8 Cr√©ation du tunnel SSH √† l'instance EC2 (Ma√Ætre)\n",
    "\n",
    "### 4.8.1 Cr√©ation des autorisations sur les connexions entrantes\n",
    "\n",
    "<u>Nous souhaitons maintenant pouvoir acc√©der √† nos applications</u> :\n",
    " - **JupyterHub** pour l'ex√©cution de notre notebook\n",
    " - **Serveur d'historique Spark** pour le suivi de l'ex√©cution <br />\n",
    "   des t√¢ches de notre script lorsqu'il sera lanc√©\n",
    " \n",
    "Cependant, <u>ces applications ne sont accessibles que depuis le r√©seau local du driver</u>, <br />\n",
    "et pour y acc√©der nous devons **cr√©er un tunnel SSH vers le driver**.\n",
    "\n",
    "Par d√©faut, ce driver se situe derri√®re un firewall qui bloque l'acc√®s en SSH. <br />\n",
    "<u>Pour ouvrir le port 22 qui correspond au port sur lequel √©coute le serveur SSH, <br />\n",
    "il faut modifier le **groupe de s√©curit√© EC2 du driver**</u>.\n",
    "\n",
    "*Il faudra que l'on se connecte en SSH au driver de notre cluster. <br />\n",
    "Par d√©faut, ce driver se situe derri√®re un firewall qui bloque l'acc√®s en SSH. <br />\n",
    "Pour ouvrir le port 22 qui correspond au port sur lequel √©coute le serveur SSH, <br />\n",
    "il faut modifier le groupe de s√©curit√© EC2 du driver. Sur la page de la console <br />\n",
    "consacr√©e √† EC2, dans l'onglet \"R√©seau et s√©curit√©\", cliquez sur \"Groupes de s√©curit√©\". <br />\n",
    "Vous allez devoir modifier le groupe de s√©curit√© d‚ÄôElasticMapReduce-Master. <br />\n",
    "Dans l'onglet \"Entrant\", ajoutez une r√®gle SSH dont la source est \"N'importe o√π\" <br />\n",
    "(ou \"Mon IP\" si vous disposez d'une adresse IP fixe).*\n",
    "\n",
    "![Configuration autorisation ports entrants pour ssh](img/EMR_config_ssh_01.png)\n",
    "\n",
    "<u>Une fois cette √©tape r√©alis√©e vous devriez avoir une configuration semblable √† la mienne</u> :\n",
    "\n",
    "![Configuration ssh termin√©e](img/EMR_config_ssh_02.png)\n",
    "\n",
    "### 4.8.2 Cr√©ation du tunnel ssh vers le Driver\n",
    "\n",
    "On peut maintenant √©tablir le **tunnel SSH** vers le **Driver**. <br />\n",
    "Pour cela on r√©cup√®re les informations de connexion fournis par Amazon <br />\n",
    "depuis la page du service EMR / Cluster / onglet R√©capitulatif en <br />\n",
    "cliquant sur \"**Activer la connexion Web**\"\n",
    "\n",
    "![Activer la connexion Web](img/EMR_tunnel_ssh_01.png)\n",
    "\n",
    "<u>On r√©cup√®re ensuite la commande fournis par Amazon pour **√©tablir le tunnel SSH**</u> :\n",
    "\n",
    "![R√©cup√©rer la commande pour √©tablir le tunnel ssh](img/EMR_tunnel_ssh_02.png)\n",
    "\n",
    "<u>Dans mon cas, la commande ne fonctionne pas tel</u> quel et j'ai du **l'adapter √† ma configuration**. <br />\n",
    "La **cl√© ssh** se situe dans un dossier \"**.ssh**\" elle-m√™me situ√©e dans <br />\n",
    "mon **r√©pertoire personnel** dont le symbole est, sous Linux, identifi√© par un tilde \"**~**\".\n",
    "\n",
    "<u>Finalement, j'utilise la commande suivante dans un terminal pour √©tablir <br />\n",
    "    mon tunnel ssh (seul l'URL change d'une instance √† une autre)</u> : <br />\n",
    "\"**ssh -i ~/.ssh/p8-ec2.pem -D 5555 hadoop@ec2-35-180-91-39.eu-west-3.compute.amazonaws.com**\"\n",
    "\n",
    "<u>On inscrit \"**yes**\" pour valider la connexion et si <br />\n",
    "    la connexion est √©tablit on obtient le r√©sultat suivant</u> :\n",
    "\n",
    "![Cr√©ation du tunnel SSH](img/EMR_connexion_ssh_01.png)\n",
    "\n",
    "Nous avons **correctement √©tabli le tunnel ssh avec le driver** sur le port \"5555\".\n",
    "\n",
    "### 4.8.3 Configuration de FoxyProxy\n",
    "\n",
    "Une derni√®re √©tape est n√©cessaire pour acc√©der √† nos applications, <br />\n",
    "en demandant √† notre navigateur d'emprunter le tunnel ssh.<br />\n",
    "J'utilise pour cela **FoxyProxy**.\n",
    "\n",
    "Sinon, ouvrez la configuration de **FoxyProxy** et <u>cliquez sur **Ajouter**</u> en haut √† gauche <br />\n",
    "puis renseigner les √©l√©ments comme dans la capture ci-dessous :\n",
    "\n",
    "![Configuration FoxyProxy Etape 1](img/EMR_foxyproxy_config_01.png)\n",
    "\n",
    "<u>On obtient le r√©sultat ci-dessous</u> :\n",
    "\n",
    "![Configuration FoxyProxy Etape 2](img/EMR_foxyproxy_config_02.png)\n",
    "\n",
    "\n",
    "### 4.8.4 Acc√®s aux applications du serveur EMR via le tunnel ssh\n",
    "\n",
    "\n",
    "<u>Avant d'√©tablir notre **tunnel ssh** nous avions √ßa</u> :\n",
    "\n",
    "![avant tunnel ssh](img/EMR_tunnel_ssh_avant.png)\n",
    "\n",
    "<u>On active le **tunnel ssh** comme vu pr√©c√©demment puis on demande <br />\n",
    "√† notre navigateur de l'utiliser avec **FoxyProxy**</u> :\n",
    "\n",
    "![FoxyProxy activation](img/EMR_foxyproxy_activation.png)\n",
    "\n",
    "<u>On peut maintenant s'apercevoir que plusieurs applications nous sont accessibles</u> :\n",
    "\n",
    "![avant tunnel ssh](img/EMR_tunnel_ssh_apres.png)\n",
    "\n",
    "## 4.9 Connexion au notebook JupyterHub\n",
    "\n",
    "Pour se connecter √† **JupyterHub** en vue d'ex√©cuter notre **notebook**, <br />\n",
    "il faut commencer par <u>cliquer sur l'application **JupyterHub**</u> apparu <br />\n",
    "depuis que nous avons configur√© le **tunnel ssh** et **foxyproxy** sur <br />\n",
    "notre navigateur (actualisez la page si ce n‚Äôest pas le cas).\n",
    "\n",
    "![D√©marrage de JupyterHub](img/EMR_jupyterhub_connexion_01.png)\n",
    "\n",
    "On passe les √©ventuels avertissements de s√©curit√© puis <br />\n",
    "nous arrivons sur une page de connexion.\n",
    "    \n",
    "<u>On se connecte avec les informations par d√©faut</u> :\n",
    " - <u>login</u>: **jovyan**\n",
    " - <u>password</u>: **jupyter**\n",
    " \n",
    "![Connexion √† JupyterHub](img/EMR_jupyterhub_connexion_02.png)\n",
    "\n",
    "Nous arrivons ensuite dans un dossier vierge de notebook.<br />\n",
    "Il suffit d'en cr√©er un en cliquant sur \"**New**\" en haut √† droite.\n",
    "\n",
    "![Liste et cr√©ation des notebook](img/EMR_jupyterhub_creer_notebooks.png)\n",
    "\n",
    "Il est √©galement possible d'en <u>uploader un directement dans notre **bucket S3**</u>.\n",
    "\n",
    "Grace √† la <u>**persistance** param√©tr√©e √† l'instanciation du cluster <br />\n",
    "nous sommes actuellement dans l'arborescence de notre **bucket S3**</u>\n",
    "\n",
    "![Notebook stock√©s sur S3](img/EMR_jupyterhub_S3.png)\n",
    "\n",
    "Je d√©cide d'**importer un notebook d√©j√† r√©dig√© en local directement <br />\n",
    "sur S3** et je l'ouvre depuis **l'interface JupyterHub**.\n",
    "\n",
    "## 4.10 Ex√©cution du code\n",
    "\n",
    "Je d√©cide d'ex√©cuter cette partie du code depuis **JupyterHub h√©berg√© sur notre cluster EMR**.<br />\n",
    "Pour ne pas alourdir inutilement les explications du **notebook**, je ne r√©expliquerai pas les √©tapes communes <br />\n",
    "que nous avons d√©j√† vues dans la premi√®re partie o√π l'on a ex√©cut√© le code localement sur notre machine virtuelle Ubuntu.\n",
    "\n",
    "<u>Avant de commencer</u>, il faut s'assurer d'utiliser le **kernel pyspark**.\n",
    "\n",
    "**En utilisant ce kernel, une session spark est cr√©√© √† l'ex√©cution de la premi√®re cellule**. <br />\n",
    "Il n'est donc **plus n√©cessaire d'ex√©cuter le code \"spark = (SparkSession ...\"** comme lors <br />\n",
    "de l'ex√©cution de notre notebook en local sur notre VM Ubuntu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e759c1e",
   "metadata": {},
   "source": [
    "### 4.10.1 D√©marrage de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5f0fbe1",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1763050717019_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-8-39.eu-west-3.compute.internal:20888/proxy/application_1763050717019_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-13-0.eu-west-3.compute.internal:8042/node/containerlogs/container_1763050717019_0002_01_000001/livy\">Link</a></td><td>None</td><td>‚úî</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# L'ex√©cution de cette cellule d√©marre l'application Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba202f",
   "metadata": {},
   "source": [
    "<u>Affichage des informations sur la session en cours et liens vers Spark UI</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb788991",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1763050717019_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-8-39.eu-west-3.compute.internal:20888/proxy/application_1763050717019_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-13-0.eu-west-3.compute.internal:8042/node/containerlogs/container_1763050717019_0002_01_000001/livy\">Link</a></td><td>None</td><td>‚úî</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac9832",
   "metadata": {},
   "source": [
    "### 4.10.2 Installation des packages\n",
    "\n",
    "Les packages n√©cessaires ont √©t√© install√© via l'√©tape de **bootstrap** √† l'instanciation du serveur.\n",
    "\n",
    "### 4.10.3 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad562eab",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83663cbd",
   "metadata": {},
   "source": [
    "### 4.10.4 D√©finition des PATH pour charger les images et enregistrer les r√©sultats\n",
    "\n",
    "Nous acc√©dons directement √† nos **donn√©es sur S3** comme si elles √©taient **stock√©es localement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46be859d",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:        s3://fruit-projet-11-feniou/\n",
      "PATH_Data:   s3://fruit-projet-11-feniou//Test1\n",
      "PATH_Result: s3://fruit-projet-11-feniou/Results"
     ]
    }
   ],
   "source": [
    "PATH = 's3://fruit-projet-11-feniou/'\n",
    "PATH_Data = PATH + '/Test1'\n",
    "PATH_Result = PATH + 'Results'\n",
    "\n",
    "print('PATH:        ' + PATH + \n",
    "      '\\nPATH_Data:   ' + PATH_Data + \n",
    "      '\\nPATH_Result: ' + PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf883c20",
   "metadata": {},
   "source": [
    "### 4.10.5 Traitement des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe93f5",
   "metadata": {},
   "source": [
    "#### 4.10.5.1 Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4b319a",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bfeb4d",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|s3://fruit-projet...|2025-11-08 20:52:54|  6894|[FF D8 FF E0 00 1...|\n",
      "|s3://fruit-projet...|2025-11-08 20:52:44|  6880|[FF D8 FF E0 00 1...|\n",
      "|s3://fruit-projet...|2025-11-08 20:52:49|  6857|[FF D8 FF E0 00 1...|\n",
      "|s3://fruit-projet...|2025-11-08 20:26:50|  6854|[FF D8 FF E0 00 1...|\n",
      "|s3://fruit-projet...|2025-11-08 20:26:49|  6849|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "images.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32ac34",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52ab808",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------------------------------------------------------------+-------------+\n",
      "|path                                                         |label        |\n",
      "+-------------------------------------------------------------+-------------+\n",
      "|s3://fruit-projet-11-feniou/Test1/Cucumber Ripe/r_165_100.jpg|Cucumber Ripe|\n",
      "|s3://fruit-projet-11-feniou/Test1/Cucumber Ripe/r_163_100.jpg|Cucumber Ripe|\n",
      "|s3://fruit-projet-11-feniou/Test1/Cucumber Ripe/r_164_100.jpg|Cucumber Ripe|\n",
      "|s3://fruit-projet-11-feniou/Test1/Cauliflower/r_183_100.jpg  |Cauliflower  |\n",
      "|s3://fruit-projet-11-feniou/Test1/Cauliflower/r_173_100.jpg  |Cauliflower  |\n",
      "+-------------------------------------------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15b199",
   "metadata": {},
   "source": [
    "#### 4.10.5.2 Pr√©paration du mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc95d05",
   "metadata": {},
   "source": [
    "#####  Changement du code \n",
    "\n",
    "- Le driver ne diffuse plus le mod√®le complet, juste les poids.\n",
    "\n",
    "- Chaque worker cr√©e son propre mod√®le √† partir de l‚Äôarchitecture vierge (weights=None).\n",
    "\n",
    "- Ensuite il charge les poids broadcast√©s (copi√©s depuis le driver).\n",
    "\n",
    "Le cluster EMR peut ex√©cuter le m√™me mod√®le en parall√®le sans recharger les fichiers depuis Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7c7165",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "\r    8192/14536120 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   16384/14536120 [..............................] - ETA: 47s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   49152/14536120 [..............................] - ETA: 42s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   81920/14536120 [..............................] - ETA: 40s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  147456/14536120 [..............................] - ETA: 28s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  212992/14536120 [..............................] - ETA: 23s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  327680/14536120 [..............................] - ETA: 17s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  475136/14536120 [..............................] - ETA: 13s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  729088/14536120 [>.............................] - ETA: 9s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1114112/14536120 [=>............................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1736704/14536120 [==>...........................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2334720/14536120 [===>..........................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3112960/14536120 [=====>........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3735552/14536120 [======>.......................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4333568/14536120 [=======>......................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5136384/14536120 [=========>....................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5799936/14536120 [==========>...................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6225920/14536120 [===========>..................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7036928/14536120 [=============>................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8011776/14536120 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8962048/14536120 [=================>............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9920512/14536120 [===================>..........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10928128/14536120 [=====================>........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r11952128/14536120 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12853248/14536120 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r13852672/14536120 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14536120/14536120 [==============================] - 2s 0us/step"
     ]
    }
   ],
   "source": [
    "# Charger MobileNetV2 pr√©-entra√Æn√© sur le driver\n",
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b9bc650",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cr√©er le nouveau mod√®le sans la couche de classification\n",
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d1a10f",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structure du mod?le :\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "# V√©rification de la structure du mod√®le\n",
    "print(\"\\nStructure du mod√®le :\")\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d497f2",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? 260 tenseurs de poids broadcast?s"
     ]
    }
   ],
   "source": [
    "broadcast_weights = sc.broadcast(new_model.get_weights())\n",
    "print(f\"‚úì {len(broadcast_weights.value)} tenseurs de poids broadcast√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8fe2b9",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    #  CORRECTION : Cr√©er le mod√®le sans t√©l√©charger les poids (weights=None)\n",
    "    model = MobileNetV2(weights=None,  # ‚Üê MODIFI√â ICI\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    new_model = Model(inputs=model.input,\n",
    "                      outputs=model.layers[-2].output)\n",
    "    \n",
    "\n",
    "    new_model.set_weights(broadcast_weights.value) \n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032f135",
   "metadata": {},
   "source": [
    "#### 4.10.5.3 D√©finition du processus de chargement des images <br/> et application de leur featurisation √† travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "933100cf",
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/yarn/usercache/livy/appcache/application_1763050717019_0002/container_1763050717019_0002_01_000001/pyspark.zip/pyspark/sql/pandas/functions.py:403: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23206e8",
   "metadata": {},
   "source": [
    "#### 4.10.5.4 Ex√©cutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d760c2",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e07fd68",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_df = images.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06a930b3",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://fruit-projet-11-feniou//Results"
     ]
    }
   ],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c53ddd5",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5555d6b5",
   "metadata": {},
   "source": [
    "#### 4.10.5.5 Ajout de l'ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c13d064a",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "No module named 'sklearn'\n",
      "Traceback (most recent call last):\n",
      "ModuleNotFoundError: No module named 'sklearn'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import des librairies n√©cessaires pour l'ACP\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe01b72",
   "metadata": {},
   "source": [
    "### 4.10.6 Chargement des donn√©es enregistr√©es et validation du r√©sultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db18a784",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d750d2a8",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                path  ...                                           features\n",
      "0  s3://fruit-projet-11-feniou/Test1/Cauliflower/...  ...  [0.0, 1.2955158, 2.6507866, 0.0, 0.0, 0.0, 0.0...\n",
      "1  s3://fruit-projet-11-feniou/Test1/Cauliflower/...  ...  [0.0, 1.16174, 2.1769102, 0.0, 0.03319602, 0.0...\n",
      "2  s3://fruit-projet-11-feniou/Test1/Cucumber Rip...  ...  [2.3874683, 0.122546375, 0.0, 0.02177108, 0.15...\n",
      "3  s3://fruit-projet-11-feniou/Test1/Beetroot/r_1...  ...  [0.3792914, 0.0, 0.0, 0.016522875, 0.0, 0.0, 0...\n",
      "4  s3://fruit-projet-11-feniou/Test1/Cantaloupe 2...  ...  [0.028503506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05757...\n",
      "\n",
      "[5 rows x 3 columns]"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b29205ff",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280,)"
     ]
    }
   ],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fba6455",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6914, 3)"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe833e3",
   "metadata": {},
   "source": [
    "#### 4.10.5.5 Ajout de l'ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e726314",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation de scikit-learn et matplotlib sur EMR...\n",
      "\n",
      "Tentative d'installation avec sudo...\n",
      "Note: \n",
      "We trust you have received the usual lecture from the local System\n",
      "Administrator. It usually boils down to these three things:\n",
      "\n",
      "    #1) Respect the privacy of others.\n",
      "    #2) Think before you type.\n",
      "    #3) With great power comes great responsibility.\n",
      "\n",
      "sudo: no tty present and no askpass program specified\n",
      "\n",
      "\n",
      "Tentative d'installation dans /tmp...\n",
      "? Packages install?s dans /tmp/python_packages\n",
      "? Pas besoin de red?marrer le kernel"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INSTALLATION DES LIBRAIRIES POUR L'ACP - VERSION EMR\n",
    "# ============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"Installation de scikit-learn et matplotlib sur EMR...\")\n",
    "\n",
    "# Option 1 : Installer avec sudo (recommand√© sur EMR)\n",
    "print(\"\\nTentative d'installation avec sudo...\")\n",
    "result = subprocess.run(\n",
    "    ['sudo', 'pip3', 'install', 'scikit-learn', 'matplotlib'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úì Packages install√©s avec succ√®s\")\n",
    "    print(\"\\n‚ö†Ô∏è IMPORTANT: Red√©marrez le kernel!\")\n",
    "    print(\"   Kernel > Restart Kernel\")\n",
    "else:\n",
    "    print(f\"Note: {result.stderr}\")\n",
    "    \n",
    "    # Option 2 : Utiliser --target pour installer dans /tmp\n",
    "    print(\"\\nTentative d'installation dans /tmp...\")\n",
    "    \n",
    "    target_dir = '/tmp/python_packages'\n",
    "    \n",
    "    # Ajouter le r√©pertoire au path\n",
    "    if target_dir not in sys.path:\n",
    "        sys.path.insert(0, target_dir)\n",
    "    \n",
    "    result2 = subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', \n",
    "         'scikit-learn', 'matplotlib', '--target', target_dir],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result2.returncode == 0:\n",
    "        print(f\"‚úì Packages install√©s dans {target_dir}\")\n",
    "        print(\"‚úì Pas besoin de red√©marrer le kernel\")\n",
    "    else:\n",
    "        print(f\"‚úó Erreur: {result2.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff2b676b",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import des librairies n√©cessaires pour l'ACP\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30f1bfda",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion des features en Vectors...\n",
      "Conversion termin?e : 6914 images\n",
      "+--------------------+--------------------+\n",
      "|                path|     features_vector|\n",
      "+--------------------+--------------------+\n",
      "|s3://fruit-projet...|[0.0,1.4010987281...|\n",
      "|s3://fruit-projet...|[0.01079463027417...|\n",
      "|s3://fruit-projet...|[0.0,0.0753617286...|\n",
      "|s3://fruit-projet...|[0.0,0.0,0.0,0.0,...|\n",
      "|s3://fruit-projet...|[0.42793282866477...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Import n√©cessaire pour la PCA\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Conversion des array en vecteur\n",
    "# PySpark ML n√©cessite que les features soient au format Vector.\n",
    "# Nous devons donc convertir nos arrays Python en Vectors Spark.\n",
    "def array_to_vector(arr):\n",
    "    \"\"\"Convertit un array Python en Vector Spark.\"\"\"\n",
    "    return Vectors.dense(arr)\n",
    "\n",
    "# Cr√©er l'UDF\n",
    "array_to_vector_udf = udf(array_to_vector, VectorUDT())\n",
    "\n",
    "# Appliquer la conversion\n",
    "print(\"Conversion des features en Vectors...\")\n",
    "features_vectors_df = features_df.withColumn(\n",
    "    \"features_vector\", \n",
    "    array_to_vector_udf(col(\"features\"))\n",
    ")\n",
    "\n",
    "print(f\"Conversion termin√©e : {features_vectors_df.count()} images\")\n",
    "features_vectors_df.select(\"path\", \"features_vector\").show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfcde9d5",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standardisation des donn√©es\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_vector\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withStd=True,   # Normaliser l'√©cart-type\n",
    "    withMean=True   # Centrer sur la moyenne\n",
    ")\n",
    "\n",
    "# Entra√Æner le scaler\n",
    "scaler_model = scaler.fit(features_vectors_df)\n",
    "\n",
    "# Transformer les donn√©es\n",
    "scaled_df = scaler_model.transform(features_vectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfe63c62",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Application de la PCA (1280 ? 50 dimensions)...\n",
      "? Entra?nement de la PCA avec k=350...\n",
      "PCA termin?e\n",
      "\n",
      "======================================================================\n",
      "ANALYSE DE LA VARIANCE EXPLIQU?E\n",
      "======================================================================\n",
      "\n",
      " Variance expliqu?e par composante :\n",
      "Composante      Variance        Cumulative     \n",
      "---------------------------------------------\n",
      "PC1             0.0992          0.0992         \n",
      "PC2             0.0590          0.1582         \n",
      "PC3             0.0531          0.2113         \n",
      "PC4             0.0312          0.2425         \n",
      "PC5             0.0305          0.2730         \n",
      "PC6             0.0251          0.2981         \n",
      "PC7             0.0243          0.3223         \n",
      "PC8             0.0238          0.3462         \n",
      "PC9             0.0225          0.3687         \n",
      "PC10            0.0214          0.3901         \n",
      "---------------------------------------------\n",
      "\n",
      " Variance totale expliqu?e par les 350 composantes : 94.48%"
     ]
    }
   ],
   "source": [
    "# Application de la PCA\n",
    "\"\"\"\n",
    "R√©duction de 1280 dimensions √† 50 dimensions.\n",
    "Le nombre k=50 est un compromis entre compression et perte d'information.\n",
    "\"\"\"\n",
    "print(\" Application de la PCA (1280 ‚Üí 50 dimensions)...\")\n",
    "\n",
    "# Configuration de la PCA\n",
    "k_components = 350  # Nombre de composantes √† garder\n",
    "pca = PCA(\n",
    "    k=k_components,\n",
    "    inputCol=\"scaled_features\",\n",
    "    outputCol=\"pca_features\"\n",
    ")\n",
    "\n",
    "# Entra√Æner la PCA\n",
    "print(f\"‚è≥ Entra√Ænement de la PCA avec k={k_components}...\")\n",
    "pca_model = pca.fit(scaled_df)\n",
    "\n",
    "# Transformer les donn√©es\n",
    "pca_df = pca_model.transform(scaled_df)\n",
    "\n",
    "print(\"PCA termin√©e\")\n",
    "\n",
    "# Analyse de la variance expliqu√©e\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSE DE LA VARIANCE EXPLIQU√âE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# R√©cup√©rer la variance expliqu√©e par chaque composante\n",
    "explained_variance = pca_model.explainedVariance\n",
    "\n",
    "# Calculer la variance cumul√©e\n",
    "cumulative_variance = 0.0\n",
    "print(\"\\n Variance expliqu√©e par composante :\")\n",
    "print(f\"{'Composante':<15} {'Variance':<15} {'Cumulative':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(min(10, k_components)):  # Afficher les 10 premi√®res\n",
    "    cumulative_variance += explained_variance[i]\n",
    "    print(f\"PC{i+1:<13} {explained_variance[i]:<15.4f} {cumulative_variance:<15.4f}\")\n",
    "\n",
    "# Variance totale\n",
    "total_variance = sum(explained_variance)\n",
    "print(\"-\" * 45)\n",
    "print(f\"\\n Variance totale expliqu√©e par les {k_components} composantes : {total_variance:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2bbbf50",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ?chantillon de r?sultats :\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                path|      features_1280d|        features_50d|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|s3://fruit-projet...|[0.0,1.3056937456...|[-1.9378421084392...|\n",
      "|s3://fruit-projet...|[1.88859724998474...|[-4.6732674988996...|\n",
      "|s3://fruit-projet...|[0.00384044647216...|[-17.026581588945...|\n",
      "|s3://fruit-projet...|[0.0,0.0216169171...|[5.08084298928730...|\n",
      "|s3://fruit-projet...|[0.0,0.0164171047...|[5.52918772250299...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "  Sauvegarde des r?sultats PCA...\n",
      " Sauvegarde Parquet : s3://fruit-projet-11-feniou/Results/pca_results.parquet"
     ]
    }
   ],
   "source": [
    "print(\"\\n √âchantillon de r√©sultats :\\n\")\n",
    "pca_df.select(\n",
    "    \"path\",\n",
    "    col(\"features_vector\").alias(\"features_1280d\"),\n",
    "    col(\"pca_features\").alias(\"features_50d\")\n",
    ").show(5, truncate=True)\n",
    "\n",
    "print(\"\\n  Sauvegarde des r√©sultats PCA...\")\n",
    "\n",
    "# Extraire le label depuis le path\n",
    "pca_df = pca_df.withColumn(\n",
    "    \"label\",\n",
    "    element_at(split(col(\"path\"), \"/\"), -2)\n",
    ")\n",
    "\n",
    "# S√©lectionner les colonnes finales\n",
    "final_df = pca_df.select(\n",
    "    \"path\",\n",
    "    \"label\",\n",
    "    \"pca_features\"\n",
    ")\n",
    "\n",
    "# Sauvegarder au format Parquet (recommand√©)\n",
    "output_path_parquet = PATH_Result + \"/pca_results.parquet\"\n",
    "print(f\" Sauvegarde Parquet : {output_path_parquet}\")\n",
    "\n",
    "final_df.write.mode(\"overwrite\").parquet(output_path_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72974aab",
   "metadata": {},
   "source": [
    "<u>On peut √©galement constater la pr√©sence des fichiers <br />\n",
    "    au format \"**parquet**\" sur le **serveur S3**</u> :\n",
    "\n",
    "![Affichage des r√©sultats sur S3](img/S3_Results.png)\n",
    "\n",
    "## 4.11 Suivi de l'avancement des t√¢ches avec le Serveur d'Historique Spark\n",
    "\n",
    "Il est possible de voir l'avancement des t√¢ches en cours <br />\n",
    "avec le **serveur d'historique Spark**.\n",
    "\n",
    "![Acc√®s au serveur d'historique spark](img/EMR_serveur_historique_spark_acces.png)\n",
    "\n",
    "**Il est √©galement possible de revenir et d'√©tudier les t√¢ches <br />\n",
    "qui ont √©t√© r√©alis√©, afin de debugger, optimiser les futurs <br />\n",
    "t√¢ches √† r√©aliser.**\n",
    "\n",
    "<u>Lorsque la commande \"**features_df.write.mode(\"overwrite\").parquet(PATH_Result)**\" <br />\n",
    "√©tait en cours, nous pouvions observer son √©tat d'avancement</u> :\n",
    "\n",
    "![Progression execution script](img/EMR_jupyterhub_avancement.png)\n",
    "\n",
    "<u>Le **serveur d'historique Spark** nous permet une vision beaucoup plus pr√©cise <br />\n",
    "de l'ex√©cution des diff√©rentes t√¢che sur les diff√©rentes machines du cluster</u> :\n",
    "\n",
    "![Suivi des t√¢ches spark](img/EMR_SHSpark_01.png)\n",
    "\n",
    "On peut √©galement constater que notre cluster de calcul a mis <br />\n",
    "un tout petit peu **moins de 8 minutes** pour traiter les **22 688 images**.\n",
    "\n",
    "![Temps de traitement](img/EMR_SHSpark_02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d65bf",
   "metadata": {},
   "source": [
    "## 4.12 R√©siliation de l'instance EMR\n",
    "\n",
    "Notre travail est maintenant termin√©. <br />\n",
    "Le cluster de machines EMR est **factur√© √† la demande**, <br />\n",
    "et nous continuons d'√™tre factur√© m√™me lorsque <br />\n",
    "les machines sont au repos.<br />\n",
    "Pour **optimiser la facturation**, il nous faut <br />\n",
    "maintenant **r√©silier le cluster**.\n",
    "\n",
    "<u>Je r√©alise cette commande depuis l'interface AWS</u> :\n",
    "\n",
    "1. Commencez par **d√©sactiver le tunnel ssh dans FoxyProxy** pour √©viter des probl√®mes de **timeout**.\n",
    "![D√©sactivation de FoxyProxy](img/EMR_foxyproxy_desactivation.png)\n",
    "2. Cliquez sur \"**R√©silier**\"\n",
    "![Cliquez sur R√©silier](img/EMR_resiliation_01.png)\n",
    "3. Confirmez la r√©siliation\n",
    "![Confirmez la r√©siliation](img/EMR_resiliation_02.png)\n",
    "4. La r√©siliation prend environ **1 minute**\n",
    "![R√©siliation en cours](img/EMR_resiliation_03.png)\n",
    "5. La r√©siliation est effectu√©e\n",
    "![R√©siliation termin√©e](img/EMR_resiliation_04.png)\n",
    "\n",
    "## 4.13 Cloner le serveur EMR (si besoin)\n",
    "\n",
    "Si nous devons de nouveau ex√©cuter notre notebook dans les m√™mes conditions, <br />\n",
    "il nous suffit de **cloner notre cluster** et ainsi en obtenir une copie fonctionnelle <br />\n",
    "sous 15/20 minutes, le temps de son instanciation.\n",
    "\n",
    "<u>Pour cela deux solutions</u> :\n",
    "1. <u>Depuis l'interface AWS</u> :\n",
    " 1. Cliquez sur \"**Cloner**\"\n",
    "   ![Cloner un cluster](img/EMR_cloner_01.png)\n",
    " 2. Dans notre cas nous ne souhaitons pas inclure d'√©tapes\n",
    "   ![Ne pas inclure d'√©tapes](img/EMR_cloner_02.png)\n",
    " 3. La configuration du cluster est recr√©√©e √† l‚Äôidentique. <br />\n",
    "    On peut revenir sur les diff√©rentes √©tapes si on souhaite apporter des modifications<br />\n",
    "    Quand tout est pr√™t, cliquez sur \"**Cr√©er un cluster**\"\n",
    "  ![V√©rification/Modification/Cr√©er un cluster](img/EMR_cloner_03.png)\n",
    "2. <u>En ligne de commande</u> (avec AWS CLI d'install√© et de configur√© et en s'assurant <br />\n",
    "   de s'attribuer les droits n√©cessaires sur le compte AMI utilis√©)\n",
    " 1. Cliquez sur \"**Exporter AWS CLI**\"\n",
    " ![Exporter AWS CLI](img/EMR_cloner_cli_01.png)\n",
    " 2. Copier/Coller la commande **depuis un terminal**\n",
    " ![Copier Coller Commande](img/EMR_cloner_cli_02.png)\n",
    "\n",
    "## 4.14 Arborescence du serveur S3 √† la fin du projet\n",
    "\n",
    "<u>Pour information, voici **l'arborescence compl√®te de mon bucket S3 p8-data** √† la fin du projet</u> : <br />\n",
    "*Par soucis de lisibilit√©, je ne liste pas les 131 sous dossiers du r√©pertoire \"Test\"*\n",
    "\n",
    "1. Results/_SUCCESS\n",
    "1. Results/part-00000-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00001-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00002-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00003-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00004-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00005-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00006-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00007-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00008-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00009-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00010-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00011-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00012-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00013-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00014-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00015-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00016-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00017-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00018-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00019-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00020-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00021-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00022-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00023-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Test/\n",
    "1. bootstrap-emr.sh\n",
    "1. jupyter-s3-conf.json\n",
    "1. jupyter/jovyan/.s3keep\n",
    "1. jupyter/jovyan/P8_01_Notebook.ipynb\n",
    "1. jupyter/jovyan/_metadata\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/file-perm.sqlite\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/html/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/latex/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbsignatures.db\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/notebook_secret\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/test3-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled1.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/test3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba46f9",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Nous avons r√©alis√© ce projet **en deux temps** en tenant <br />\n",
    "compte des contraintes qui nous ont √©t√© impos√©es.\n",
    "\n",
    "Nous avons **dans un premier temps d√©velopp√© notre solution en local** <br />\n",
    "sur une machine virtuelle dans un environnement Linux Ubuntu.\n",
    "\n",
    "La <u>premi√®re phase</u> a consist√© √† **installer l'environnement de travail Spark**. <br />\n",
    "**Spark** a un param√®tre qui nous permet de travaill√© en local et nous permet <br />\n",
    "ainsi de **simuler du calcul partag√©** en consid√©rant <br />\n",
    "**chaque c≈ìur d'un processeur comme un worker ind√©pendant**.<br />\n",
    "Nous avons travaill√© sur un plus **petit jeu de donn√©e**, l'id√©e √©tait <br />\n",
    "simplement de **valider le bon fonctionnement de la solution**.\n",
    "\n",
    "Nous avons fait le choix de r√©aliser du **transfert learning** <br />\n",
    "√† partir du model **MobileNetV2**.<br />\n",
    "Ce mod√®le a √©t√© retenu pour sa **l√©g√®ret√©** et sa **rapidit√© d'ex√©cution** <br />\n",
    "ainsi que pour la **faible dimension de son vecteur en sortie**.\n",
    "\n",
    "Les r√©sultats ont √©t√© enregistr√©s sur disque en plusieurs <br />\n",
    "partitions au format \"**parquet**\".\n",
    "\n",
    "<u>**La solution a parfaitement fonctionn√© en mode local**</u>.\n",
    "\n",
    "La <u>deuxi√®me phase</u> a consist√© √† cr√©er un **r√©el cluster de calculs**. <br />\n",
    "L'objectif √©tait de pouvoir **anticiper une future augmentation de la charge de travail**.\n",
    "\n",
    "Le meilleur choix retenu a √©t√© l'utilisation du prestataire de services **Amazon Web Services** <br />\n",
    "qui nous permet de **louer √† la demande de la puissance de calculs**, <br />\n",
    "pour un **co√ªt tout √† fait acceptable**.<br />\n",
    "Ce service se nomme **EC2** et se classe parmi les offres **Infrastructure As A Service** (IAAS).\n",
    "\n",
    "Nous sommes allez plus loin en utilisant un service de plus <br />\n",
    "haut niveau (**Plateforme As A Service** PAAS)<br />\n",
    "en utilisant le service **EMR** qui nous permet d'un seul coup <br />\n",
    "d'**instancier plusieurs serveur (un cluster)** sur lesquels <br />\n",
    "nous avons pu demander l'installation et la configuration de plusieurs<br />\n",
    "programmes et librairies n√©cessaires √† notre projet comme **Spark**, <br />\n",
    "**Hadoop**, **JupyterHub** ainsi que la librairie **TensorFlow**.\n",
    "\n",
    "En plus d'√™tre plus **rapide et efficace √† mettre en place**, nous avons <br />\n",
    "la **certitude du bon fonctionnement de la solution**, celle-ci ayant √©t√© <br />\n",
    "pr√©alablement valid√© par les ing√©nieurs d'Amazon.\n",
    "\n",
    "Nous avons √©galement pu installer, sans difficult√©, **les packages <br />\n",
    "n√©cessaires sur l'ensembles des machines du cluster**.\n",
    "\n",
    "Enfin, avec tr√®s peu de modification, et plus simplement encore, <br />\n",
    "nous avons pu **ex√©cuter notre notebook comme nous l'avions fait localement**.<br />\n",
    "Nous avons cette fois-ci ex√©cut√© le traitement sur **l'ensemble des images de notre dossier \"Test\"**.\n",
    "\n",
    "Nous avons opt√© pour le service **Amazon S3** pour **stocker les donn√©es de notre projet**. <br />\n",
    "S3 offre, pour un faible co√ªt, toutes les conditions dont nous avons besoin pour stocker <br />\n",
    "et exploiter de mani√®re efficace nos donn√©es.<br />\n",
    "L'espace allou√© est potentiellement **illimit√©**, mais les co√ªts seront fonction de l'espace utilis√©.\n",
    "\n",
    "Il nous sera **facile de faire face √† une mont√© de la charge de travail** en **redimensionnant** <br />\n",
    "simplement notre cluster de machines (horizontalement et/ou verticalement au besoin), <br />\n",
    "les co√ªts augmenteront en cons√©quence mais resteront nettement inf√©rieurs aux co√ªts engendr√©s <br />\n",
    "par l'achat de mat√©riels ou par la location de serveurs d√©di√©s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
